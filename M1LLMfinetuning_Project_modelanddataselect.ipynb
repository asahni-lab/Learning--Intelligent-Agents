{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a644ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: torch in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.2 MB/s  0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install pandas torch transformers scikit-learn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf623321",
   "metadata": {},
   "source": [
    "# Step-by-step guide to preprocessing data\n",
    "\n",
    "This reading will guide you through the following steps:\n",
    "\n",
    "- **Step 1:** Data Preprocessing\n",
    "- **Step 2:** Clean the text \n",
    "- **Step 3:** Tokenize\n",
    "- **Step 4:** Handle missing data\n",
    "- **Step 5:** Prepare the data for fine-tuning\n",
    "- **Step 6:** Split the data\n",
    "\n",
    "## Step 1: Data Preprocessing\n",
    "\n",
    "Before diving into the cleaning and tokenization processes, it's essential to import and organize the raw data into a structured format. We begin by loading the dataset, defining necessary labels, and preparing the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f025343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "      id                                              tweet    class  \\\n",
      "0  40815  Loved @Bethenny independence msg on @WendyWill...     fear   \n",
      "1  10128  @mark_slifer actually maybe we were supposed t...  sadness   \n",
      "2  40476  I thought the nausea and headaches had passed ...     fear   \n",
      "3  20813  Anger, resentment, and hatred are the destroye...    anger   \n",
      "4  40796  new tires &amp; an alarm system on my car. fwm...     fear   \n",
      "\n",
      "  sentiment_intensity class_intensity  labels  \n",
      "0                 low        fear_low       4  \n",
      "1                high    sadness_high       9  \n",
      "2              medium     fear_medium       5  \n",
      "3                high      anger_high       0  \n",
      "4                 low        fear_low       4  \n",
      "\n",
      "Total samples: 3960\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the dataset from the URL\n",
    "url = \"https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/resolve/main/train.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing for this specific dataset:\n",
    "# 1. Rename 'tweet' to 'text' so it works with our cleaning function later\n",
    "if 'tweet' in data.columns:\n",
    "    data = data.rename(columns={'tweet': 'text'})\n",
    "\n",
    "# 2. Create numeric labels from the 'emotion' column\n",
    "if 'emotion' in data.columns:\n",
    "    # Create a mapping from emotion string to number (e.g., anger -> 0, fear -> 1)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(data['emotion'].unique())}\n",
    "    data['label'] = data['emotion'].map(label_mapping)\n",
    "    print(f\"\\nLabel mapping applied: {label_mapping}\")\n",
    "\n",
    "# Convert labels to PyTorch tensor\n",
    "if 'label' in data.columns:\n",
    "    labels = torch.tensor(data['label'].tolist())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c62a37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4639ed",
   "metadata": {},
   "source": [
    "## Step 2: Clean the text\n",
    "\n",
    "Text cleaning is the first step in preparing your dataset. It involves removing unwanted characters, URLs, and excess whitespace to ensure uniformity and cleanliness in the data. Text is also changed to lowercase to maintain consistency across all data points.\n",
    "\n",
    "**Explanation:**\n",
    "Cleaning the text by removing unnecessary characters and formatting it ensures that the data is consistent, making it easier for the model to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ab1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    loved bethenny independence msg on wendywillia...\n",
      "1    mark_slifer actually maybe we were supposed to...\n",
      "2    i thought the nausea and headaches had passed ...\n",
      "3    anger resentment and hatred are the destroyer ...\n",
      "4      new tires amp an alarm system on my car fwm now\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to your dataset\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "print(data['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0411f50",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize\n",
    "\n",
    "Tokenization is the process of converting text into individual tokens that a machine-learning model can understand. We use the tokenizer corresponding to the pretrained model (e.g., BERT) for this. This ensures that the data is properly formatted and ready for fine-tuning.\n",
    "\n",
    "**Explanation:**\n",
    "Tokenization converts the cleaned text into a format suitable for fine-tuning the model, ensuring that the input is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc80993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  3866,  7014,  2368,  4890,  4336,  5796,  2290,  2006, 12815,\n",
      "         29602,  6632,  5244,  2022,  3407, 23713, 16829,  2306,  4426, 23713,\n",
      "         13433, 28032,  7730,  2097, 19311,  2000,  2017,  3407,  2981,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2928,  1035, 22889, 23780,  2941,  2672,  2057,  2020,  4011,\n",
      "          2000,  3280,  1998,  2026, 13445,  5552,  2256,  3268, 27451,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  1045,  2245,  1996, 19029,  1998, 14978,  2015,  2018,  2979,\n",
      "          2021,  8840,  2140,  1045,  2514,  9643,  2651,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  4963, 20234,  1998, 11150,  2024,  1996,  9799,  1997,  2115,\n",
      "          7280,  2651,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  2047, 13310, 23713,  2019,  8598,  2291,  2006,  2026,  2482,\n",
      "          1042,  2860,  2213,  2085,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Check the first 5 tokenized examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59163227",
   "metadata": {},
   "source": [
    "### Step 4: Handle missing data\n",
    "\n",
    "Missing data is common in real-world datasets. You can handle it either by removing incomplete entries or by imputing missing values. This step is critical to preventing errors during the training process.\n",
    "\n",
    "**Explanation:**\n",
    "Handling missing data ensures that your dataset is complete, which prevents training interruptions or biases introduced by missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02f2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     0\n",
      "text                   0\n",
      "class                  0\n",
      "sentiment_intensity    0\n",
      "class_intensity        0\n",
      "labels                 0\n",
      "cleaned_text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Option 1: Drop rows with missing data\n",
    "# CRITICAL: We comment this out because dropping rows changes the dataframe length,\n",
    "# but our 'tokens' variable (created in Step 3) still has the original length.\n",
    "# This would cause a mismatch error in Step 5.\n",
    "# data = data.dropna() \n",
    "\n",
    "# Option 2: Fill missing values with a placeholder (Safer)\n",
    "data['cleaned_text'] = data['cleaned_text'].fillna('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b095e67",
   "metadata": {},
   "source": [
    "#### Explanation of code\n",
    "\n",
    "- **`synonym_replacement`**: This function uses the `nltk` libraryâ€™s wordnet to retrieve synonyms of a given word. If synonyms are available, it randomly selects one. If not, the original word is returned.\n",
    "- **`augment_text`**: This function iterates through each word in the text, replacing it with a synonym based on a random probability (here, a 20 percent chance for each word).\n",
    "- **Applying augmentation**: We apply the `augment_text` function to the cleaned text in the dataset, creating a new column, `augmented_text`, which contains the augmented text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "110f0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amansahni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amansahni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\amansahni\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: loved bethenny independence msg on wendywilliams be happy amp fulfilled within yourself amp positivity will flock to you happy independent\n",
      "Augmented: loved bethenny independence msg on wendywilliams be happy amp carry_through within yourself amp positivity will flock to you happy independent\n",
      "Augmented: loved bethenny independence msg on wendywilliams be happy amp carry_through within yourself amp positivity will flock to you happy independent\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import random # Random module for generating random numbers and selections\n",
    "import nltk\n",
    "from nltk.corpus import wordnet # NLTK's WordNet corpus for finding synonyms\n",
    "\n",
    "# Download WordNet data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Define a function to find and replace a word with a synonym\n",
    "def synonym_replacement(word):\n",
    "    # Get all synsets (sets of synonyms) for the given word from WordNet\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    \n",
    "    # If the word has synonyms, randomly choose one synonym, otherwise return the original word\n",
    "    if synonyms:\n",
    "        synonym = synonyms[0].lemmas()[0].name()\n",
    "        return synonym\n",
    "    return word\n",
    "\n",
    "def augment_text(text):\n",
    "    words = text.split()\n",
    "    new_words = [synonym_replacement(word) if random.random() < 0.2 else word for word in words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Apply augmentation to create a new column\n",
    "# We apply this to a small subset or the whole dataset depending on needs\n",
    "# For demonstration, let's apply it to the first 5 rows\n",
    "print(\"Original:\", data['cleaned_text'].iloc[0])\n",
    "print(\"Augmented:\", augment_text(data['cleaned_text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b583c",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In certain cases, especially when data is limited, data augmentation techniques can be applied to generate new training examples by modifying the original dataset.\n",
    "\n",
    "- **Paraphrasing:** Rewriting sentences in different ways while preserving the meaning.\n",
    "- **Backtranslation:** Translating text into another language and back again to create variation.\n",
    "- **Synonym replacement:** Replacing certain words in the text with their synonyms.\n",
    "\n",
    "#### Code example for synonym replacement (augmentation)\n",
    "\n",
    "The following example demonstrates how to implement synonym replacement using the `nltk` library. It randomly replaces words in the text with their synonyms to create new variations of sentences. This method can be applied when paraphrasing or backtranslation is not feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850a3ad",
   "metadata": {},
   "source": [
    "### Step 5: Prepare the data for fine-tuning\n",
    "\n",
    "After cleaning and tokenizing your text, the next step is to prepare the data for fine-tuning. This involves structuring the tokenized data and labels into a format suitable for training, such as PyTorch DataLoader objects.\n",
    "\n",
    "# Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40aec7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'intensity' column not found. Using 'label' column if available.\n",
      "DataLoader created successfully!\n",
      "DataLoader created successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define a mapping function for sentiment intensity\n",
    "def map_sentiment(value):\n",
    "    if value == \"high\":\n",
    "        return 1.0\n",
    "    elif value == \"medium\":\n",
    "        return 0.5\n",
    "    elif value == \"low\":\n",
    "        return 0.0\n",
    "    return None\n",
    "\n",
    "# Apply the mapping function to the 'intensity' column (assuming it exists in this dataset)\n",
    "# Note: The dataset loaded in Step 1 has an 'intensity' column.\n",
    "if 'intensity' in data.columns:\n",
    "    data['sentiment_intensity'] = data['intensity'].apply(map_sentiment)\n",
    "    \n",
    "    # Drop rows where sentiment_intensity is None (invalid values)\n",
    "    data = data.dropna(subset=['sentiment_intensity'])\n",
    "    \n",
    "    # Convert to tensor\n",
    "    labels = torch.tensor(data['sentiment_intensity'].tolist())\n",
    "    print(\"Labels created from 'intensity' column.\")\n",
    "else:\n",
    "    # Fallback if 'intensity' column is missing (using previous logic)\n",
    "    print(\"'intensity' column not found. Using 'label' column if available.\")\n",
    "    if 'label' in data.columns:\n",
    "         labels = torch.tensor(data['label'].tolist())\n",
    "\n",
    "# Re-tokenize to ensure alignment after potential dropna\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    return_tensors='pt', \n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "\n",
    "# Create a DataLoader for training\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"DataLoader created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19477e5b",
   "metadata": {},
   "source": [
    "### Step 6: Split the data\n",
    "\n",
    "Before training, itâ€™s important to split your data into training, validation, and test sets. The training set is used to train the model, the validation set helps to tune model hyperparameters, and the test set is used for final evaluation to ensure that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db5143fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader objects for training, validation, and test sets created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 15% for test set, the rest for training/validation\n",
    "train_val_inputs, test_inputs, train_val_masks, test_masks, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 20% for validation set from remaining data\n",
    "train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_masks, train_val_labels, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader objects for training, validation, and test sets\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"DataLoader objects for training, validation, and test sets created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ca9e9",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The `train_test_split` method from the `sklearn.model_selection` module splits your data into training and validation (or test) sets. Here's a breakdown of how it works:\n",
    "\n",
    "- **`input_ids` and `labels`**: These are the inputs and labels you are splitting.\n",
    "- **`test_size=0.1`**: This indicates that 10 percent of the data will be set aside for the test set.\n",
    "- **`random_state=42`**: This ensures the split is reproducibleâ€”using the same random state will produce the same split every time.\n",
    "\n",
    "In this case, we first split the data into two sets:\n",
    "1. **`train_val_inputs` and `test_inputs`**: A combined set of training + validation data and a test set.\n",
    "\n",
    "Then, we further split the `train_val_inputs` into `train_inputs` and `val_inputs` to get a separate validation set.\n",
    "\n",
    "This process allows us to train, validate, and test data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Following this walkthrough, youâ€™ve cleaned, tokenized, and structured your dataset for fine-tuning. With clean and well-prepared data, your model will have the best chance of achieving high performance during fine-tuning. You can use these preprocessing steps in your machine-learning projects to ensure optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35382fda",
   "metadata": {},
   "source": [
    "## Step 7: Fine-tune the Model\n",
    "\n",
    "Now that our data is prepared, we can proceed to fine-tune the BERT model. We will use the `Trainer` API from Hugging Face, which simplifies the training loop.\n",
    "\n",
    "**Key Components:**\n",
    "- **Model:** We load `bert-base-uncased` with a classification head.\n",
    "- **Training Arguments:** We define hyperparameters like learning rate, batch size, and number of epochs.\n",
    "- **Trainer:** Handles the training and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db55b406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 1:07:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=507, training_loss=0.004114732400677672, metrics={'train_runtime': 4045.7894, 'train_samples_per_second': 1.996, 'train_steps_per_second': 0.125, 'total_flos': 178455526558488.0, 'train_loss': 0.004114732400677672, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. Load the model\n",
    "# We are doing regression (predicting a float value 0.0 - 1.0), so num_labels=1\n",
    "# Explicitly set problem_type=\"regression\" to ensure correct loss function (MSELoss)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1, problem_type=\"regression\")\n",
    "\n",
    "# 2. Define Metrics Function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.flatten()\n",
    "    \n",
    "    # For regression, we can use Mean Squared Error (MSE)\n",
    "    mse = ((preds - labels) ** 2).mean()\n",
    "    return {\n",
    "        'mse': mse,\n",
    "    }\n",
    "\n",
    "# 3. Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",           # Evaluate every epoch\n",
    "    save_strategy=\"epoch\",           # Save checkpoint every epoch\n",
    "    load_best_model_at_end=True,     # Load the best model when finished\n",
    ")\n",
    "\n",
    "# 4. Initialize Trainer\n",
    "# Note: We need to wrap our TensorDatasets into a format Trainer expects (dictionary with keys)\n",
    "# Or simpler: Use the standard Dataset object which we didn't use earlier. \n",
    "# Since we have TensorDatasets, we can write a small wrapper or just use a custom loop.\n",
    "# HOWEVER, for simplicity and standard practice, let's convert our Tensors back to HF Datasets quickly.\n",
    "\n",
    "def create_hf_dataset(inputs, masks, labels):\n",
    "    # Ensure labels are float32 for regression\n",
    "    # Ensure inputs are long for embedding layers\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': inputs,\n",
    "        'attention_mask': masks,\n",
    "        'labels': labels.to(torch.float32) \n",
    "    })\n",
    "\n",
    "train_hf_ds = create_hf_dataset(train_inputs, train_masks, train_labels)\n",
    "val_hf_ds = create_hf_dataset(val_inputs, val_masks, val_labels)\n",
    "test_hf_ds = create_hf_dataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_hf_ds,           # training dataset\n",
    "    eval_dataset=val_hf_ds,              # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "# 5. Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c64c1",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the Model\n",
    "\n",
    "After training, it is crucial to evaluate the model's performance to ensure it has learned effectively and is not overfitting. We will visualize the training loss and validation metrics.\n",
    "\n",
    "**What to look for:**\n",
    "- **Training Loss:** Should decrease over time.\n",
    "- **Validation Loss (MSE):** Should also decrease. If it starts rising, the model might be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Extract training history\n",
    "history = trainer.state.log_history\n",
    "df_history = pd.DataFrame(history)\n",
    "\n",
    "# Filter for training loss and eval loss\n",
    "train_loss = df_history[df_history['loss'].notna()][['epoch', 'loss']]\n",
    "eval_loss = df_history[df_history['eval_loss'].notna()][['epoch', 'eval_loss']]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss['epoch'], train_loss['loss'], label='Training Loss')\n",
    "plt.plot(eval_loss['epoch'], eval_loss['eval_loss'], label='Validation Loss (MSE)', marker='o')\n",
    "\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Final Evaluation on Test Set\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "test_results = trainer.evaluate(test_hf_ds)\n",
    "print(f\"Test MSE: {test_results['eval_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f175c68",
   "metadata": {},
   "source": [
    "## Step 9: Test with Random Examples\n",
    "\n",
    "Finally, let's see the model in action! We will pick a few random examples from the test set, print the text, the actual intensity, and the model's predicted intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to predict intensity for a single text\n",
    "def predict_intensity(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    # Move to same device as model\n",
    "    inputs = {k: v.to(trainer.model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**inputs)\n",
    "    \n",
    "    # Get prediction (it's a regression value)\n",
    "    prediction = outputs.logits.item()\n",
    "    return prediction\n",
    "\n",
    "# Pick 5 random samples from the original dataframe (to get the text back easily)\n",
    "# We use the 'test_inputs' indices if we tracked them, but for simplicity, let's just pick random texts \n",
    "# and see what the model thinks, comparing to our intuition.\n",
    "\n",
    "print(\"--- Random Prediction Showcase ---\")\n",
    "samples = data.sample(5)\n",
    "\n",
    "for index, row in samples.iterrows():\n",
    "    text = row['cleaned_text']\n",
    "    actual_intensity = row['sentiment_intensity']\n",
    "    predicted_intensity = predict_intensity(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Actual Intensity: {actual_intensity}\")\n",
    "    print(f\"Predicted Intensity: {predicted_intensity:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
