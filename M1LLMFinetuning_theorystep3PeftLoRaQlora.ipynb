{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b85928f",
   "metadata": {},
   "source": [
    "# Practice Activity: Applying PEFT\n",
    "\n",
    "## Introduction\n",
    "Parameter-efficient fine-tuning (PEFT) is a technique that reduces the computational cost and memory requirements of fine-tuning large pretrained models. Instead of updating all of the model’s parameters, PEFT focuses on fine-tuning a smaller subset of the model’s parameters while keeping most of the model's original weights frozen. This approach allows for faster training times and lower memory usage, making fine-tuning more feasible for large-scale models.\n",
    "\n",
    "In this reading, we'll explore the key steps for applying PEFT to a pretrained model and the benefits of using this technique.\n",
    "\n",
    "By the end of this activity, you will be able to:\n",
    "\n",
    "*   Understand the concept of PEFT and its advantages.\n",
    "*   Identify key parameters for fine-tuning and apply the PEFT technique to a pretrained model.\n",
    "*   Implement a fine-tuning process with reduced computational cost and memory usage.\n",
    "*   Evaluate and optimise the performance of the fine-tuned model using PEFT.\n",
    "\n",
    "## Why Use PEFT?\n",
    "Traditional fine-tuning methods require updating all of the model’s parameters, which can be computationally expensive, especially for large models such as GPT-3, BERT, or T5. PEFT offers several benefits:\n",
    "\n",
    "*   **Reduced computational cost:** By only fine-tuning a subset of the model’s parameters, you can significantly reduce the amount of computational resources needed.\n",
    "*   **Lower memory requirements:** PEFT uses less memory since only a few parameters are updated, making it easier to fine-tune on smaller GPUs or machines with limited resources.\n",
    "*   **Faster training times:** With fewer parameters to update, the training process is much faster, allowing for quicker iterations and experiments.\n",
    "\n",
    "## Step-by-Step Process for Applying PEFT\n",
    "This reading will guide you through the following steps:\n",
    "\n",
    "1.  Prepare your data and identify the subset of parameters for fine-tuning\n",
    "2.  Set up fine-tuning with PEFT\n",
    "3.  Monitor and evaluate performance\n",
    "4.  Optimise PEFT for your task\n",
    "\n",
    "## Step 1: Prepare Your Data and Identify the Subset of Parameters for Fine-Tuning\n",
    "Before beginning the fine-tuning process, ensure that your dataset is properly prepared. You should be working with a task-specific dataset (e.g., sentiment analysis, text classification) that aligns with the pretrained model you’ll be using. Preprocess the data, ensuring it’s tokenised and ready for input into the model. For this activity, we’ll assume you’re working with a classification task, but this process can also be adapted for other tasks.\n",
    "\n",
    "**Instructions for Preparing Your Data:**\n",
    "\n",
    "*   Ensure that your dataset is cleaned and preprocessed.\n",
    "*   Tokenise the data using a tokenizer compatible with the pretrained model (e.g., BERT tokenizer for a BERT model).\n",
    "*   Split your dataset into training, validation, and test sets.\n",
    "\n",
    "Once your data is ready, the next step is identifying which parameters to fine-tune. In PEFT, we often fine-tune the parameters in the task-specific heads, which are the layers responsible for generating predictions based on the task. For models like BERT, the task-specific heads are the final few layers, usually the classification head.\n",
    "\n",
    "**Locate the Task-Specific Heads:**\n",
    "In a BERT-based model, task-specific heads typically refer to the layers at the end of the model used for tasks such as classification, where the model generates outputs based on the input data.\n",
    "You can inspect the model architecture to find these heads and determine which layers are responsible for your task.\n",
    "\n",
    "**Approach:**\n",
    "To implement PEFT, you will freeze most of the model’s parameters, allowing only the parameters in the task-specific heads (final layers) to be updated. This strategy minimises computational cost while allowing the model to adapt to your specific task.\n",
    "\n",
    "**Customise Fine-Tuning:**\n",
    "You can also choose to fine-tune multiple layers if your task requires more adaptation. For example, you might fine-tune the last two or three layers instead of just the final classification head. This gives you more flexibility in training while still taking advantage of the efficiency of PEFT.\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Load pre-trained BERT model\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Step 1: Freeze all layers except the last one (classification head)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "This loop freezes all the layers except for the final classification head. If you wish to fine-tune more than just the last layer, you can modify the loop to unfreeze the last two or three layers for retraining.\n",
    "\n",
    "## Step 2: Set Up Fine-Tuning with PEFT\n",
    "Once you've identified the fine-tuning parameters, you can set up the process. For this example, we will use the Hugging Face Transformers library, which provides an easy interface for model fine-tuning.\n",
    "\n",
    "**Instructions for Fine-Tuning with PEFT:**\n",
    "\n",
    "*   Freeze the layers of the model (as shown in the previous code block).\n",
    "*   Set up the fine-tuning process using Hugging Face’s `Trainer` class and `TrainingArguments`.\n",
    "*   Fine-tune the model based on the trainer setup.\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Step 1: Set training arguments for fine-tuning the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # Directory where results will be stored\n",
    "    num_train_epochs=3,                 # Number of epochs (full passes through the dataset)\n",
    "    per_device_train_batch_size=16,     # Batch size per GPU/CPU during training\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate the model at the end of each epoch\n",
    ")\n",
    "```\n",
    "\n",
    "**Note:**\n",
    "\n",
    "*   The `Trainer` class from Hugging Face is responsible for setting up the fine-tuning process.\n",
    "*   The line `trainer.train()` fine-tunes the model with PEFT, leveraging the frozen layers from Step 1.\n",
    "\n",
    "## Step 3: Monitor and Evaluate Performance\n",
    "After fine-tuning the model with PEFT, it is important to evaluate the model's performance and compare it to traditional fine-tuning methods. PEFT achieves similar or even better performance with less computational cost.\n",
    "\n",
    "**Evaluation:**\n",
    "Use standard evaluation metrics (e.g., accuracy, F1 score) to monitor the fine-tuned model's performance on the validation and test sets.\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate(eval_dataset=test_data)\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']}\")\n",
    "```\n",
    "\n",
    "## Step 4: Optimise PEFT for Your Task\n",
    "PEFT can be further optimised for specific tasks by experimenting with different sets of parameters or layers to fine-tune. You can also try adjusting the learning rate or batch size to see how they impact the model’s performance.\n",
    "\n",
    "**Optimisation Ideas:**\n",
    "\n",
    "*   Fine-tune additional layers (e.g., the last two to three layers instead of just the final classification head).\n",
    "*   Adjust hyperparameters such as learning rate and number of epochs to find the best configuration for your task.\n",
    "\n",
    "**Code Example:**\n",
    "```python\n",
    "# Example of adjusting learning rate for PEFT optimisation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=5e-5,  # Experiment with different learning rates\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    ")\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "PEFT is an efficient method for fine-tuning large pretrained models, allowing you to save computational resources and time without sacrificing performance. By focusing on fine-tuning a subset of parameters, you can achieve task-specific improvements while keeping the rest of the model intact. This makes PEFT particularly useful when hardware resources are limited or rapid experimentation is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b831e",
   "metadata": {},
   "source": [
    "# Practice activity: Applying QLoRA\n",
    "\n",
    "## Introduction\n",
    "Quantized Low-Rank Adaptation (QLoRA) is a cutting-edge fine-tuning technique designed to reduce memory and computational requirements while maintaining model performance drastically. It builds on Low-Rank Adaptation (LoRA) principles but adds quantization to the process, further reducing the size of the model’s weight matrices. This allows even large-scale language models to be fine-tuned on smaller hardware, making them accessible for more practical use cases.\n",
    "\n",
    "In this reading, we’ll explore how QLoRA works, its advantages, and the steps to apply it effectively to fine-tune pretrained models.\n",
    "\n",
    "By the end of this reading, you will be able to:\n",
    "- Describe how QLoRA combines quantization and low-rank adaptation for efficient fine-tuning.\n",
    "- Apply QLoRA to a pretrained model to reduce memory and computational costs.\n",
    "- Fine-tune a quantized low-rank model on task-specific data and evaluate its performance.\n",
    "- Optimize QLoRA for specific tasks by adjusting quantization levels and rank values.\n",
    "\n",
    "## Why use QLoRA?\n",
    "Traditional fine-tuning approaches require updating all the parameters in a model, which can be resource-intensive, especially for large models. LoRA addresses this issue by introducing low-rank adaptations, but even LoRA can require significant memory for very large models. QLoRA enhances the fine-tuning process by applying quantization, which reduces the precision of the model's weights (e.g., from 32-bit to 8-bit or even 4-bit), lowering the memory and computational requirements. Quantizing a model involves approximating the model's weight values to lower-precision numbers, significantly reducing the memory footprint while preserving much of the model's performance. This makes fine-tuning feasible on smaller hardware such as consumer graphics processing units (GPUs).\n",
    "\n",
    "### Benefits of QLoRA\n",
    "- **Lower memory requirements**: by quantizing model parameters, QLoRA reduces the memory needed for storing and processing large models.\n",
    "- **Reduced computational costs**: similar to LoRA, QLoRA reduces the number of parameters that need to be fine-tuned. Quantization further reduces the computational burden.\n",
    "- **Faster training**: QLoRA allows for faster fine-tuning due to its smaller memory and computational requirements, making it ideal for rapid iterations.\n",
    "\n",
    "## Step-by-step guide to fine-tune with QLoRA\n",
    "The remaining of this reading will guide you through the following steps:\n",
    "1. Step 1: Data setup for QLoRA fine-tuning\n",
    "2. Step 2: Apply QLoRA to a pretrained model\n",
    "3. Step 3: Fine-tune the QLoRA-enhanced model\n",
    "4. Step 4: Evaluate the QLoRA-fine-tuned model\n",
    "5. Step 5: Optimize QLoRA for specific tasks\n",
    "\n",
    "### Step 1: Data setup for QLoRA fine-tuning\n",
    "To begin fine-tuning using QLoRA, you must set up your data properly. This includes preparing the dataset by splitting it into training, validation, and test sets. This step is crucial for ensuring that the model is trained effectively and can generalize well to unseen data.\n",
    "\n",
    "**Steps**\n",
    "- Collect or load the dataset you want to use for fine-tuning.\n",
    "- Split the dataset into training (for model learning), validation (for tuning hyperparameters), and test sets (for evaluating performance).\n",
    "- Preprocess the data by tokenizing it, ensuring that it aligns with the input format expected by the model.\n",
    "\n",
    "### Step 2: Apply QLoRA to a pretrained model\n",
    "To apply QLoRA, you need to quantize the model and apply low-rank adaptations to specific layers, such as attention layers or feed-forward networks. QLoRA modifies these layers while keeping the rest of the model frozen.\n",
    "\n",
    "In most cases, QLoRA allows you to choose which layers to quantize. You can experiment by quantizing only certain layers, such as the attention layers or feed-forward networks, rather than quantizing all layers. This flexibility allows you to explore different configurations and adjust the quantization to fit your specific task.\n",
    "\n",
    "Both GPT-2 and BERT are pretrained transformer models widely used for natural language processing tasks. While GPT-2 is a generative model focusing on text generation, and BERT is optimized for tasks such as classification and question answering, they share a similar architecture based on the transformer model. This makes them both suitable candidates for QLoRA, demonstrating how the method can be applied to a variety of pretrained models.\n",
    "\n",
    "**Steps**\n",
    "- Load a pretrained model (e.g., GPT-2, BERT).\n",
    "- Quantize the model to reduce precision.\n",
    "- Apply LoRA to specific layers.\n",
    "- Fine-tune the quantized low-rank matrices while freezing the rest of the parameters.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "from qlora import QuantizeModel, LoRALayer\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = QuantizeModel(model, bits=8)\n",
    "\n",
    "# Apply LoRA to specific layers (e.g., attention layers)\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "In this example, the pretrained GPT-2 model is quantized to 8 bits, drastically reducing its memory requirements. LoRA is then applied to specific layers, such as attention heads, to ensure that only a small subset of parameters is fine-tuned.\n",
    "\n",
    "### Step 3: Fine-tune the QLoRA-enhanced model\n",
    "Once QLoRA is applied, the fine-tuning process begins. You will fine-tune the quantized model's low-rank matrices on your task-specific dataset, allowing the model to adapt to the task efficiently.\n",
    "\n",
    "**Steps**\n",
    "- Prepare the dataset by splitting it into training, validation, and test sets.\n",
    "- Fine-tune the model using only the quantized low-rank matrices.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "The model is fine-tuned using the Trainer API, but only the quantized low-rank matrices are updated during training, making the process more efficient compared to traditional fine-tuning.\n",
    "\n",
    "### Step 4: Evaluate the QLoRA-fine-tuned model\n",
    "After fine-tuning, it’s important to evaluate the model’s performance on the test set to determine how well it generalizes to unseen data. While quantization can sometimes introduce small performance trade-offs, QLoRA aims to balance efficiency with high performance.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate(eval_dataset=test_data)\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']}\")\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "After fine-tuning, the model is evaluated using the test set. Standard evaluation metrics such as accuracy, precision, recall, and F1 score can be used to assess the model’s performance.\n",
    "\n",
    "### Step 5: Optimize QLoRA for specific tasks\n",
    "You can optimize QLoRA by adjusting the rank of the low-rank matrices or experimenting with different quantization levels. You can find the best balance between model efficiency and performance for your specific task by tuning these parameters.\n",
    "\n",
    "**Optimization ideas**\n",
    "- Adjust the rank of the low-rank matrices (e.g., increasing or decreasing the rank).\n",
    "- Experiment with different quantization levels (e.g., 4-bit or 8-bit quantization) to see how they affect the model’s performance.\n",
    "- Consider experimenting with other parameters, such as dropout rate, learning rate, or layer-wise adaptation, to see how they influence fine-tuning results. This provides additional flexibility in customizing the model for task-specific requirements.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "from qlora import adjust_qlora_rank\n",
    "\n",
    "# Adjust the rank of the low-rank matrices\n",
    "adjust_qlora_rank(quantized_model, rank=4)  # Experiment with different rank values\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "QLoRA is an advanced fine-tuning technique that combines the benefits of quantization and low-rank adaptation. By reducing the memory and computational requirements, QLoRA makes it feasible to fine-tune large models even on consumer-grade hardware. With careful application, QLoRA can deliver efficient fine-tuning without sacrificing performance, making it ideal for resource-constrained environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7a2c5",
   "metadata": {},
   "source": [
    "# Practice activity: Applying LoRA\n",
    "\n",
    "**Disclaimer:**\n",
    "Azure libraries are regularly updated, and changes may occasionally affect the behavior of this exercise. If you experience any issues, consider rolling back the affected library to an earlier version to maintain compatibility. Always refer to official Microsoft documentation for the most current guidance.\n",
    "\n",
    "## Introduction\n",
    "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning technique that allows us to adapt large pretrained models to specific tasks with a substantial reduction in computational and memory costs. Instead of adjusting all model parameters, LoRA applies low-rank matrix modifications to key layers, such as attention heads, which means only a small subset of parameters needs to be fine-tuned. This method makes LoRA ideal for adapting large models to task-specific data without the significant resource demands of full model fine-tuning. In this reading, we’ll examine how LoRA functions, the steps for implementing it, and its benefits for fine-tuning large language models efficiently.\n",
    "\n",
    "By the end of this reading, you will be able to:\n",
    "- Describe the key concepts and benefits of low-rank adaptation (LoRA) in fine-tuning large models.\n",
    "- Apply LoRA to a pretrained model for task-specific fine-tuning.\n",
    "- Fine-tune a model using LoRA with minimized computational and memory resources.\n",
    "- Evaluate and optimize the performance of a LoRA-fine-tuned model.\n",
    "\n",
    "## Why use LoRA?\n",
    "Traditional fine-tuning methods require adjusting all the parameters in a model, which is resource-intensive, especially for large transformer-based models like BERT, RoBERTa, and GPT. As models grow larger, the computational and memory costs of full fine-tuning increase substantially. LoRA addresses these challenges by applying low-rank adaptations within specific layers, focusing on fine-tuning only a subset of parameters that represent a low-rank approximation of the original model's weight matrices. The benefits of LoRA include the following:\n",
    "\n",
    "- **Reduced memory usage**: LoRA drastically reduces the memory footprint by fine-tuning only low-rank matrices rather than all model parameters, making it ideal for environments with limited memory capacity.\n",
    "- **Lower computational cost**: since fewer parameters are being optimized, LoRA requires less computation, reducing both time and energy consumption.\n",
    "- **Faster training and experimentation**: with fewer parameters to update, LoRA shortens training time, enabling faster experimentation and quicker iterations for model improvement.\n",
    "\n",
    "LoRA is particularly advantageous when working with large models in environments with constrained resources, such as edge devices or research environments in which computational budgets are limited. It also makes fine-tuning large models more feasible for a broader range of applications without requiring access to powerful hardware.\n",
    "\n",
    "## Step-by-step process to fine-tune a model using LoRA\n",
    "The remainder of this reading will guide you through the following steps:\n",
    "1. Step 1: Prepare your dataset.\n",
    "2. Step 2: Apply LoRA to the model.\n",
    "3. Step 3: Fine-tune the model with LoRA.\n",
    "4. Step 4: Evaluate the LoRA-fine-tuned model.\n",
    "5. Step 5: Optimize LoRA for your task.\n",
    "\n",
    "### Step 1: Prepare your dataset\n",
    "Before you can fine-tune a model using LoRA, it’s essential to ensure that your dataset is preprocessed and structured correctly. Proper dataset preparation is key to achieving reliable performance during fine-tuning and evaluation.\n",
    "\n",
    "**Instructions**\n",
    "- **Clean and preprocess the data**: remove irrelevant entries, handle missing values, and standardize the text as needed to ensure the data is ready for processing.\n",
    "- **Tokenize the data**: use a tokenizer compatible with your chosen model (e.g., a BERT tokenizer for BERT models). This step prepares the text for input into the model.\n",
    "- **Split the dataset**: divide the dataset into training, validation, and test sets to allow for reliable performance evaluation. A typical split is 70 percent for training, 15 percent for validation, and 15 percent for testing.\n",
    "\n",
    "By preparing the dataset carefully, you enable efficient fine-tuning and ensure that your model has access to high-quality, representative data for learning task-specific patterns.\n",
    "\n",
    "### Step 2: Apply LoRA to the model\n",
    "Once you have prepared your dataset, you can modify specific layers of a pretrained model using LoRA. The goal is to introduce low-rank matrices to key layers, often the attention layers in transformer models. This modification allows you to fine-tune only the parameters of the low-rank matrices while keeping the rest of the model frozen, significantly reducing computational requirements.\n",
    "\n",
    "**Instructions for preparation**\n",
    "- **Ensure dataset readiness**: confirm that you have preprocessed and tokenized the dataset as outlined in Step 1.\n",
    "- **Understand the model’s architecture**: review the structure of the model you’re working with, typically a transformer such as BERT or GPT, to identify layers where you can apply LoRA.\n",
    "- **Identify relevant layers**: in transformer-based models, attention layers are often the primary targets for LoRA because they manage most of the information flow in these architectures. By printing out the model’s named modules, you can identify the specific attention layers where LoRA can be introduced. These layers typically have \"attention\" in their names.\n",
    "\n",
    "**Approach**\n",
    "- **Load the pretrained model**: start with a pretrained model such as BERT to leverage its existing language understanding capabilities.\n",
    "- **Apply LoRA to attention layers**: use a LoRA-specific function, such as LoRALayer, to modify only the attention layers.\n",
    "- **Freeze remaining parameters**: freeze all other parameters in the model to ensure that only the LoRA-modified layers are adjusted during training.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "from lora import LoRALayer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load a pre-trained BERT model for classification tasks\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Print model layers to identify attention layers where LoRA can be applied\n",
    "for name, module in model.named_modules():\n",
    "    print(name)  # This output helps locate attention layers\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `print(name)`: prints each model component to help locate the attention layers where LoRA can be applied.\n",
    "- `module.apply(LoRALayer)`: applies the LoRA modification to the identified attention layers.\n",
    "- `param.requires_grad = False`: ensures all other parameters remain frozen, meaning only LoRA-modified layers will be fine-tuned.\n",
    "\n",
    "This setup enables a targeted fine-tuning approach, in which only specific, low-rank parameters are adjusted, minimizing resource use.\n",
    "\n",
    "### Step 3: Fine-tune the model with LoRA\n",
    "With LoRA applied to specific layers, you’re ready to fine-tune the model on your task-specific dataset. The goal is to update only the low-rank matrices in the attention layers, optimizing them for the task while keeping the rest of the model’s parameters static.\n",
    "\n",
    "**Approach**\n",
    "- **Start training**: fine-tune the model using the prepared dataset from Step 1.\n",
    "- **Monitor progress**: use the validation dataset to track the model’s performance during training.\n",
    "- **Focus on LoRA layers**: since LoRA was applied to the attention layers, only the low-rank matrices in these layers will be updated during training, reducing overall computational demand.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Configure training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `TrainingArguments(...)`: specifies key training parameters, such as the number of epochs, batch size, and evaluation frequency.\n",
    "- `Trainer(...)`: initializes the trainer, linking it to the model, training arguments, and datasets.\n",
    "- `trainer.train()`: starts the fine-tuning process, which updates only LoRA-modified layers.\n",
    "\n",
    "By focusing on just the low-rank matrices, you achieve efficient task-specific fine-tuning without the overhead of updating the entire model.\n",
    "\n",
    "### Step 4: Evaluate the LoRA-fine-tuned model\n",
    "After fine-tuning, evaluate the model’s performance using standard metrics such as accuracy, F1 score, and precision/recall. Since LoRA optimizes only a small subset of parameters, memory and computational costs are reduced, yet the model can still deliver performance that rivals traditional fine-tuning.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "# Evaluate the LoRA fine-tuned model on the test set\n",
    "results = trainer.evaluate(eval_dataset=test_data)\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']}\")\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `trainer.evaluate(...)`: runs an evaluation on the test dataset.\n",
    "- `results['eval_accuracy']`: retrieves the test accuracy, indicating how well the model generalizes to unseen data.\n",
    "\n",
    "This evaluation step confirms the model’s effectiveness and highlights the efficiency gains from fine-tuning only low-rank matrices, which helps maintain strong performance despite reduced computational overhead.\n",
    "\n",
    "### Step 5: Optimize LoRA for your task\n",
    "To achieve even better results, consider experimenting with the rank of the low-rank matrices in LoRA. By adjusting the rank, you can control the number of parameters in the low-rank matrices, balancing the trade-off between computational efficiency and model performance. A higher rank can capture more complexity but may require additional resources, while a lower rank further reduces resource demands.\n",
    "\n",
    "**Optimization ideas**\n",
    "- **Adjust the rank**: experiment with different ranks in the low-rank matrices to find an optimal balance for your specific task.\n",
    "- **Extend LoRA application**: apply LoRA to additional layers to capture more complex task-specific features.\n",
    "\n",
    "**Code example**\n",
    "```python\n",
    "# Example of adjusting the rank in LoRA\n",
    "from lora import adjust_lora_rank\n",
    "\n",
    "# Set a lower rank for fine-tuning, experiment with values for optimal performance\n",
    "adjust_lora_rank(model, rank=2)\n",
    "```\n",
    "\n",
    "**Explanation**\n",
    "- `adjust_lora_rank(model, rank=2)`: sets a lower rank for LoRA, which further reduces the number of parameters involved in fine-tuning, allowing for experiments with different ranks to optimize performance.\n",
    "\n",
    "This fine-tuning adjustment enables you to fine-tune LoRA-modified layers more precisely, helping the model balance resource use with performance more effectively.\n",
    "\n",
    "## Conclusion\n",
    "LoRA provides a resource-efficient alternative to traditional full model fine-tuning, allowing large pretrained models to be tailored to specific tasks with a fraction of the computational cost. By fine-tuning only low-rank approximations within key layers, LoRA enables significant reductions in memory and computational demands while retaining effective performance. This technique is particularly valuable for applications in resource-constrained environments or when experimenting with large models on specialized tasks. By following this guide, you have learned how to apply LoRA to fine-tune models efficiently, making it feasible to leverage powerful language models in various real-world applications without the prohibitive resource requirements typically associated with full fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3f310",
   "metadata": {},
   "source": [
    "# Evaluating fine-tuned models\n",
    "\n",
    "## Introduction\n",
    "After fine-tuning a pretrained model, it is critical to evaluate its performance on a task-specific dataset. Evaluation helps determine how well the model has adapted to the new task and whether it can generalize to unseen data. In this reading, we will cover key metrics and methods for evaluating fine-tuned models, including accuracy, precision, recall, and F1 score.\n",
    "\n",
    "By the end of this reading, you will be able to:\n",
    "- Explain the importance of evaluating fine-tuned models on unseen data.\n",
    "- Use key evaluation metrics such as accuracy, precision, recall, and F1 score to assess model performance.\n",
    "- Recognize signs of overfitting and underfitting during fine-tuning.\n",
    "- Compare the effectiveness of different fine-tuning techniques, including traditional fine-tuning, LoRA, and QLoRA.\n",
    "- Optimize the trade-off between performance and resource efficiency when evaluating fine-tuning methods.\n",
    "\n",
    "## Why evaluation matters\n",
    "The goal of fine-tuning is to adapt a general-purpose, pretrained model to perform well on a specific task. However, even after fine-tuning, there is no guarantee that the model will perform optimally. Evaluating the model is necessary to:\n",
    "- Ensure the model can generalize to unseen data.\n",
    "- Identify potential overfitting or underfitting issues.\n",
    "- Compare the performance of different fine-tuning techniques, such as traditional fine-tuning, LoRA, or QLoRA.\n",
    "\n",
    "## Key metrics for evaluation\n",
    "\n",
    "### Accuracy\n",
    "Accuracy measures the proportion of correctly predicted instances out of the total instances. This is a common metric for classification tasks.\n",
    "\n",
    "**Formula:**\n",
    "$$Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "**Example:**\n",
    "Imagine a binary classification task with 100 instances:\n",
    "- Correct predictions: 80\n",
    "- Total predictions: 100\n",
    "\n",
    "$$Accuracy = \\frac{80}{100} = 0.8 \\text{ (or } 80\\%)$$\n",
    "\n",
    "**When to use:** Accuracy is useful when class distribution is balanced and the cost of false positives and false negatives is roughly the same.\n",
    "\n",
    "### Precision\n",
    "Precision measures how many of the model's positive predictions are actually correct. Precision is especially useful when false positives are costly.\n",
    "\n",
    "**Formula:**\n",
    "$$Precision = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "**Example:**\n",
    "Imagine a binary classification task with 100 instances:\n",
    "- Correct predictions (True Positives): 80\n",
    "- False Positives: 20\n",
    "\n",
    "$$Precision = \\frac{80}{80 + 20} = \\frac{80}{100} = 0.8$$\n",
    "\n",
    "**When to use:** Precision is important in tasks in which minimizing false positives is more critical than false negatives, such as spam detection or fraud detection.\n",
    "\n",
    "### Recall\n",
    "Recall (also known as sensitivity) measures how many actual positives the model successfully identifies. It is particularly useful when minimizing false negatives is essential.\n",
    "\n",
    "**Formula:**\n",
    "$$Recall = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "**Example:**\n",
    "In a spam detection system:\n",
    "- True positives: 70 (emails correctly classified as spam)\n",
    "- False negatives: 20 (spam emails incorrectly classified as non-spam)\n",
    "\n",
    "$$Recall = \\frac{70}{70 + 20} = \\frac{70}{90} \\approx 0.778$$\n",
    "\n",
    "**When to use:**\n",
    "Recall is essential in tasks in which false negatives are more critical than false positives.\n",
    "Examples:\n",
    "- **Spam detection:** it avoids incorrectly flagging important emails as spam (though usually precision is prioritized here, recall ensures we catch spam). *Note: The text says \"False positives are more critical\" under \"When to use\" for Precision, and lists Spam detection there. For Recall, it usually applies to medical diagnosis or safety critical tasks.*\n",
    "\n",
    "### F1 score\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balanced metric when both false positives and false negatives matter.\n",
    "\n",
    "**Formula:**\n",
    "$$F1 = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right)$$\n",
    "\n",
    "**When to use:** The F1 score is useful when you need a balance between precision and recall, particularly in imbalanced datasets.\n",
    "\n",
    "### Confusion matrix\n",
    "A confusion matrix shows the true positives, true negatives, false positives, and false negatives in a table format. It helps visualize the model’s performance.\n",
    "\n",
    "| | Predicted positive | Predicted negative |\n",
    "|---|---|---|\n",
    "| **Actual positive** | True positive (TP) | False negative (FN) |\n",
    "| **Actual negative** | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**When to use:** A confusion matrix is valuable for understanding where the model is making errors and identifying class imbalances.\n",
    "\n",
    "## Evaluating performance on unseen data\n",
    "When evaluating fine-tuned models, it is crucial to test their performance on a test set that the model has not seen during training or validation. This provides an unbiased measure of the model’s ability to generalize to real-world data.\n",
    "\n",
    "- **Validation set:** used during fine-tuning to tune hyperparameters and monitor performance.\n",
    "- **Test set:** used after fine-tuning to evaluate the model’s final performance on unseen data.\n",
    "\n",
    "## Overfitting and underfitting\n",
    "One of the key risks during fine-tuning is overfitting or underfitting the model.\n",
    "\n",
    "### Overfitting\n",
    "Overfitting occurs when the model performs well on the training data but poorly on unseen data. This happens when the model has memorized the training set instead of learning features that generalize to new data.\n",
    "\n",
    "- **Signs of overfitting:** High accuracy on the training set but low accuracy on the validation or test sets.\n",
    "- **Solutions:** Use regularization techniques (e.g., dropout), reduce model complexity, or use data augmentation.\n",
    "\n",
    "### Underfitting\n",
    "Underfitting happens when the model performs poorly on both the training and test data. This indicates that the model is too simple to capture the underlying patterns in the data.\n",
    "\n",
    "- **Signs of underfitting:** Low accuracy on both training and test sets.\n",
    "- **Solutions:** Increase the complexity of the model, provide more training data, or train for more epochs.\n",
    "\n",
    "## Comparing techniques: Traditional fine-tuning, LoRA, and QLoRA\n",
    "When comparing the performance of different fine-tuning techniques, you should consider both the model’s performance metrics and the resource efficiency of each technique.\n",
    "\n",
    "- **Performance:** compare accuracy, F1 score, precision, and recall across techniques.\n",
    "- **Efficiency:** consider training time, memory usage, and computational cost.\n",
    "\n",
    "For example, if comparing traditional fine-tuning, LoRA, and QLoRA techniques:\n",
    "- **Traditional fine-tuning:** typically achieves high performance but requires significant memory and time.\n",
    "- **LoRA:** reduces memory usage by fine-tuning only low-rank matrices, often without a major loss in performance.\n",
    "- **QLoRA:** combines quantization with low-rank adaptation, further reducing memory usage while maintaining competitive performance.\n",
    "\n",
    "## Conclusion\n",
    "Evaluating fine-tuned models is a critical step in understanding their performance and generalization ability. By using a combination of such metrics as accuracy, precision, recall, and F1 score, you can get a comprehensive picture of how well your model performs on the task at hand. It’s also important to assess the model’s resource efficiency, particularly when comparing different fine-tuning techniques such as LoRA and QLoRA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c87f6",
   "metadata": {},
   "source": [
    "# Detailed explanation of evaluation metrics\n",
    "\n",
    "## Introduction\n",
    "When evaluating a fine-tuned model, it is essential to use appropriate metrics to understand its performance. Different metrics can provide insights into various aspects of model behavior, such as its ability to classify correctly, its sensitivity to certain classes, and how well it generalizes to new data. In this reading, we will take a detailed look at the most commonly used evaluation metrics: accuracy, precision, recall, F1 score, and others such as the confusion matrix, receiver operating characteristic–area under the curve (ROC-AUC), loss, and specificity.\n",
    "\n",
    "By the end of this reading, you will be able to:\n",
    "- Explain the importance of evaluating fine-tuned models using different metrics.\n",
    "- Identify when to use various metrics, such as accuracy, precision, recall, and F1 score, based on the specific goals of the task.\n",
    "- Interpret confusion matrices and ROC-AUC curves to visualize model performance.\n",
    "- Explain loss and specificity metrics and how they relate to model learning.\n",
    "- Select the most appropriate evaluation metrics for balanced, imbalanced, or cost-sensitive tasks.\n",
    "\n",
    "## Evaluation metrics explained\n",
    "Explore the following evaluation metrics:\n",
    "- Evaluation Metric 1: Accuracy\n",
    "- Evaluation Metric 2: Precision\n",
    "- Evaluation Metric 3: Recall (sensitivity or true positive rate)\n",
    "- Evaluation Metric 4: F1 score\n",
    "- Evaluation Metric 5: Confusion matrix\n",
    "- Evaluation Metric 6: Specificity (true negative rate)\n",
    "- Evaluation Metric 7: ROC-AUC\n",
    "- Evaluation Metric 8: Loss\n",
    "\n",
    "### Evaluation Metric 1: Accuracy\n",
    "**Definition**\n",
    "Accuracy measures the proportion of correctly classified instances out of the total number of instances. It is the most straightforward metric for classification tasks.\n",
    "\n",
    "**Formula**\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}$$\n",
    "\n",
    "Where:\n",
    "- **TP (true positives):** correct positive predictions\n",
    "- **TN (true negatives):** correct negative predictions\n",
    "- **FP (false positives):** incorrect positive predictions\n",
    "- **FN (false negatives):** incorrect negative predictions\n",
    "\n",
    "**When to use**\n",
    "Accuracy is a good metric when the class distribution is balanced. However, in the case of imbalanced datasets (in which one class occurs far more frequently than another), accuracy can be misleading. For example, in a dataset in which 90 percent of instances are of class A and only 10 percent are of class B, a model that always predicts class A would achieve 90 percent accuracy, but it would perform poorly for class B.\n",
    "\n",
    "### Evaluation Metric 2: Precision\n",
    "**Definition**\n",
    "Precision measures how many of the model's positive predictions are actually correct. It is useful when false positives are particularly costly (e.g., in spam or fraud detection).\n",
    "\n",
    "**Formula**\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**When to use**\n",
    "Precision is important when the cost of a false positive is high, meaning it’s better to be cautious when predicting positives. A high precision means fewer false positives, but it doesn’t account for false negatives.\n",
    "\n",
    "### Evaluation Metric 3: Recall (sensitivity or true positive rate)\n",
    "**Definition**\n",
    "Recall measures how many of the actual positives in the dataset the model correctly identifies. It is essential in scenarios where missing positives (false negatives) can have serious consequences (e.g., in medical diagnosis or fraud detection).\n",
    "\n",
    "**Formula**\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**When to use**\n",
    "Recall is crucial when the goal is to capture as many positive instances as possible, even if that means allowing some false positives. For example, in a cancer detection model, it’s more important to catch as many true cases of cancer as possible, even if some false positives are included.\n",
    "\n",
    "### Evaluation Metric 4: F1 score\n",
    "**Definition**\n",
    "The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. It’s useful when you want to find a middle ground between precision and recall, particularly in cases of imbalanced datasets.\n",
    "\n",
    "**Formula**\n",
    "$$F1 = 2 \\times \\left( \\frac{Precision \\times Recall}{Precision + Recall} \\right)$$\n",
    "\n",
    "**When to use**\n",
    "The F1 score is most helpful when both precision and recall are important, and there is a need to balance the two. For example, in a fraud detection system, we want to minimize both false positives (to avoid unnecessary investigations) and false negatives (to catch as much fraud as possible).\n",
    "\n",
    "### Evaluation Metric 5: Confusion matrix\n",
    "**Definition**\n",
    "A confusion matrix is a table that allows you to visualize the performance of a classification model by comparing actual versus predicted values. It provides a detailed breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "**Example**\n",
    "\n",
    "| | Predicted positive | Predicted negative |\n",
    "|---|---|---|\n",
    "| **Actual positive** | True positive | False negative |\n",
    "| **Actual negative** | False positive | True negative |\n",
    "\n",
    "**When to use**\n",
    "A confusion matrix is especially useful when analyzing a model’s errors. For example, in a binary classification problem, you can easily see whether the model is making more false positives or false negatives, helping you to adjust the model accordingly.\n",
    "\n",
    "### Evaluation Metric 6: Specificity (true negative rate)\n",
    "**Definition**\n",
    "Specificity measures the proportion of actual negatives that the model correctly identifies. It is the opposite of recall, as it focuses on how well the model avoids false positives.\n",
    "\n",
    "**Formula**\n",
    "$$Specificity = \\frac{True Negatives}{True Negatives + False Positives}$$\n",
    "\n",
    "**When to use**\n",
    "Specificity is useful when the cost of false positives is high. For example, in certain medical tests, it’s important to minimize the number of false positives to avoid unnecessary treatments.\n",
    "\n",
    "### Evaluation Metric 7: ROC-AUC\n",
    "**Definition**\n",
    "ROC-AUC measures the trade-off between the TP rate (recall) and the FP rate (1 - specificity) across different threshold values. The ROC curve plots the TP rate against the FP rate, and the area under the curve (AUC) quantifies the overall ability of the model to distinguish between classes.\n",
    "\n",
    "**When to use**\n",
    "ROC-AUC is a robust metric for evaluating binary classifiers, particularly when you want to compare how well different models perform at distinguishing between the positive and negative classes. It is often used when dealing with imbalanced datasets.\n",
    "\n",
    "- **ROC curve:** the curve itself shows the trade-off between sensitivity (recall) and specificity (true negative rate).\n",
    "- **AUC value:** an AUC of 1.0 indicates a perfect classifier, while an AUC of 0.5 indicates a model with no discriminatory ability.\n",
    "\n",
    "### Evaluation Metric 8: Loss\n",
    "**Definition**\n",
    "Loss measures how well the model’s predictions align with the actual labels. During training, the goal is to minimize the loss to improve the model's performance. Two common loss functions are cross-entropy loss (for classification problems) and mean squared error (for regression tasks).\n",
    "\n",
    "#### 1. Cross-entropy loss\n",
    "**Definition**\n",
    "Cross-entropy loss measures the difference between the predicted probabilities and the actual class labels in classification tasks. It penalizes confident but incorrect predictions more heavily.\n",
    "\n",
    "**Formula**\n",
    "$$Cross\\text{-}Entropy\\ Loss = - \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the true label (1 if the class is correct; 0 otherwise).\n",
    "- $\\hat{y}_i$ is the predicted probability for the true class.\n",
    "- $N$ is the number of classes.\n",
    "\n",
    "Cross-entropy is particularly useful for multi-class classification problems, as it ensures that the model outputs probabilities close to the true class label.\n",
    "\n",
    "#### 2. Mean squared error (MSE)\n",
    "**Definition**\n",
    "MSE is used in regression tasks and measures the average of the squared differences between predicted values and actual values.\n",
    "\n",
    "**Formula**\n",
    "$$MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ is the true value.\n",
    "- $\\hat{y}_i$ is the predicted value.\n",
    "- $n$ is the number of data points.\n",
    "\n",
    "MSE gives more weight to larger errors, meaning the model is penalized more for predictions significantly off the actual values.\n",
    "\n",
    "**When to use**\n",
    "Loss is important during the training phase because it gives insight into how well the model is learning from the data. However, it is less interpretable as a standalone metric after training compared to accuracy or the F1 score.\n",
    "\n",
    "## Choosing the right metric\n",
    "The choice of evaluation metric depends on the task and the goals of the model. Here are some guidelines:\n",
    "- For balanced datasets, accuracy is a reasonable choice.\n",
    "- For imbalanced datasets, use precision, recall, F1 score, or ROC-AUC to get a more nuanced view of model performance.\n",
    "- When false positives are costly, precision and specificity are crucial.\n",
    "- When false negatives are costly, recall is the key metric.\n",
    "- When both false positives and false negatives matter, the F1 score provides a balanced evaluation.\n",
    "\n",
    "## Conclusion\n",
    "Different evaluation metrics provide various insights into a model’s performance. By understanding these metrics, you can make better decisions about how to interpret the results of a fine-tuned model and how to improve its performance in future iterations. Always choose metrics that align with the task’s goals and the specific costs associated with false positives or false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c677660",
   "metadata": {},
   "source": [
    "# Summary: Beyond Accuracy - Evaluation Metrics for Machine Learning\n",
    "\n",
    "## Introduction\n",
    "In real-world machine learning applications, such as fraud detection or medical diagnosis, relying solely on model accuracy can be deceptive. This summary explores why high accuracy does not always equate to a successful model and details alternative metrics—Precision, Recall, and F1 Score—that are critical for assessing performance in mission-critical tasks.\n",
    "\n",
    "## The Limitation of Accuracy\n",
    "While accuracy is a common starting point for evaluation, it can be highly misleading, particularly with **imbalanced datasets**.\n",
    "*   **The Fraud Detection Paradox:** In a scenario where only 1% of transactions are fraudulent, a model that simply predicts \"not fraud\" for every single transaction achieves 99% accuracy. However, this model is practically useless because it fails to detect any actual fraud.\n",
    "*   **Insight:** Accuracy measures overall correctness but fails to capture the nuance of specific error types (false positives vs. false negatives), which is often where the real business or safety value lies.\n",
    "\n",
    "## Key Metrics Explained\n",
    "\n",
    "### 1. Precision\n",
    "*   **Definition:** Measures the accuracy of positive predictions.\n",
    "*   **Use Case:** Critical when **false positives are costly**.\n",
    "*   **Example:** In fraud detection, low precision means many legitimate transactions are flagged as fraud. Investigating these false alarms is expensive and frustrates users, so high precision is preferred.\n",
    "\n",
    "### 2. Recall\n",
    "*   **Definition:** Measures the ability of the model to find all the relevant cases (positive instances).\n",
    "*   **Use Case:** Critical when **false negatives are dangerous**.\n",
    "*   **Example:** In medical diagnoses (e.g., cancer detection), missing a positive case (a false negative) can be life-threatening. Therefore, high recall is prioritized to ensure as many cases as possible are caught, even if it means accepting some false positives.\n",
    "\n",
    "### 3. F1 Score\n",
    "*   **Definition:** The harmonic mean of Precision and Recall.\n",
    "*   **Use Case:** Best used when you need a **balance** between Precision and Recall, and when false positives and false negatives are both important. It prevents a model from being biased too heavily toward one metric at the expense of the other.\n",
    "\n",
    "## Advanced Metrics & Tools\n",
    "\n",
    "### ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "*   Helps understand how well a model distinguishes between classes.\n",
    "*   Particularly useful for assessing performance on imbalanced datasets.\n",
    "\n",
    "### Confusion Matrix\n",
    "*   A visualization tool that breaks down predictions into True Positives, True Negatives, False Positives, and False Negatives.\n",
    "*   Allows for a granular analysis of exactly *where* the model is making mistakes.\n",
    "\n",
    "## Strategic Selection of Metrics\n",
    "Choosing the right metric is not a one-size-fits-all decision; it depends entirely on the specific goals of the task and the cost of errors.\n",
    "\n",
    "| Task | Priority | Recommended Metric | Reason |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Cancer Detection** | Catching every case | **Recall** | The cost of missing a diagnosis (False Negative) is extremely high. |\n",
    "| **Fraud Detection** | Minimizing false alarms | **Precision** | The cost of blocking legitimate users (False Positive) is high. |\n",
    "| **General Classification** | Balanced performance | **F1 Score** | When both error types are undesirable. |\n",
    "\n",
    "## Conclusion\n",
    "To build models that serve real-world goals, developers must look beyond accuracy. By understanding the nuances of Precision, Recall, and F1 Score, and utilizing tools like the Confusion Matrix, practitioners can fine-tune models to minimize the specific errors that matter most to their application.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
