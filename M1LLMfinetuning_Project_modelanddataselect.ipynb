{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf623321",
   "metadata": {},
   "source": [
    "# Step-by-step guide to preprocessing data\n",
    "\n",
    "This reading will guide you through the following steps:\n",
    "\n",
    "- **Step 1:** Data Preprocessing\n",
    "- **Step 2:** Clean the text \n",
    "- **Step 3:** Tokenize\n",
    "- **Step 4:** Handle missing data\n",
    "- **Step 5:** Prepare the data for fine-tuning\n",
    "- **Step 6:** Split the data\n",
    "\n",
    "## Step 1: Data Preprocessing\n",
    "\n",
    "Before diving into the cleaning and tokenization processes, it's essential to import and organize the raw data into a structured format. We begin by loading the dataset, defining necessary labels, and preparing the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the dataset from the URL\n",
    "url = \"https://huggingface.co/datasets/stepp1/tweet_emotion_intensity/resolve/main/train.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(data.head())\n",
    "\n",
    "# Preprocessing for this specific dataset:\n",
    "# 1. Rename 'tweet' to 'text' so it works with our cleaning function later\n",
    "if 'tweet' in data.columns:\n",
    "    data = data.rename(columns={'tweet': 'text'})\n",
    "\n",
    "# 2. Create numeric labels from the 'emotion' column\n",
    "if 'emotion' in data.columns:\n",
    "    # Create a mapping from emotion string to number (e.g., anger -> 0, fear -> 1)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(data['emotion'].unique())}\n",
    "    data['label'] = data['emotion'].map(label_mapping)\n",
    "    print(f\"\\nLabel mapping applied: {label_mapping}\")\n",
    "\n",
    "# Convert labels to PyTorch tensor\n",
    "if 'label' in data.columns:\n",
    "    labels = torch.tensor(data['label'].tolist())\n",
    "\n",
    "print(f\"\\nTotal samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c62a37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4639ed",
   "metadata": {},
   "source": [
    "## Step 2: Clean the text\n",
    "\n",
    "Text cleaning is the first step in preparing your dataset. It involves removing unwanted characters, URLs, and excess whitespace to ensure uniformity and cleanliness in the data. Text is also changed to lowercase to maintain consistency across all data points.\n",
    "\n",
    "**Explanation:**\n",
    "Cleaning the text by removing unnecessary characters and formatting it ensures that the data is consistent, making it easier for the model to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to your dataset\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "print(data['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0411f50",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize\n",
    "\n",
    "Tokenization is the process of converting text into individual tokens that a machine-learning model can understand. We use the tokenizer corresponding to the pretrained model (e.g., BERT) for this. This ensures that the data is properly formatted and ready for fine-tuning.\n",
    "\n",
    "**Explanation:**\n",
    "Tokenization converts the cleaned text into a format suitable for fine-tuning the model, ensuring that the input is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc80993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = tokenizer(\n",
    "    data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128\n",
    ")\n",
    "\n",
    "print(tokens['input_ids'][:5])  # Check the first 5 tokenized examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59163227",
   "metadata": {},
   "source": [
    "### Step 4: Handle missing data\n",
    "\n",
    "Missing data is common in real-world datasets. You can handle it either by removing incomplete entries or by imputing missing values. This step is critical to preventing errors during the training process.\n",
    "\n",
    "**Explanation:**\n",
    "Handling missing data ensures that your dataset is complete, which prevents training interruptions or biases introduced by missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Option 1: Drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# Option 2: Fill missing values with a placeholder\n",
    "data['cleaned_text'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850a3ad",
   "metadata": {},
   "source": [
    "### Step 5: Prepare the data for fine-tuning\n",
    "\n",
    "After cleaning and tokenizing your text, the next step is to prepare the data for fine-tuning. This involves structuring the tokenized data and labels into a format suitable for training, such as PyTorch DataLoader objects.\n",
    "\n",
    "# Example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aec7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create PyTorch tensors from the tokenized data\n",
    "input_ids = tokens['input_ids']\n",
    "attention_masks = tokens['attention_mask']\n",
    "labels = torch.tensor(data['label'].tolist())\n",
    "\n",
    "# Create a DataLoader for training\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "print(\"DataLoader created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19477e5b",
   "metadata": {},
   "source": [
    "### Step 6: Split the data\n",
    "\n",
    "Before training, it’s important to split your data into training, validation, and test sets. The training set is used to train the model, the validation set helps to tune model hyperparameters, and the test set is used for final evaluation to ensure that the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split data into a combined training + validation set and a test set\n",
    "train_val_inputs, test_inputs, train_val_labels, test_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Now, split the combined set into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    train_val_inputs, train_val_labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader objects for training, validation, and test sets\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(\"DataLoader objects for training, validation, and test sets created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ca9e9",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The `train_test_split` method from the `sklearn.model_selection` module splits your data into training and validation (or test) sets. Here's a breakdown of how it works:\n",
    "\n",
    "- **`input_ids` and `labels`**: These are the inputs and labels you are splitting.\n",
    "- **`test_size=0.1`**: This indicates that 10 percent of the data will be set aside for the test set.\n",
    "- **`random_state=42`**: This ensures the split is reproducible—using the same random state will produce the same split every time.\n",
    "\n",
    "In this case, we first split the data into two sets:\n",
    "1. **`train_val_inputs` and `test_inputs`**: A combined set of training + validation data and a test set.\n",
    "\n",
    "Then, we further split the `train_val_inputs` into `train_inputs` and `val_inputs` to get a separate validation set.\n",
    "\n",
    "This process allows us to train, validate, and test data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Following this walkthrough, you’ve cleaned, tokenized, and structured your dataset for fine-tuning. With clean and well-prepared data, your model will have the best chance of achieving high performance during fine-tuning. You can use these preprocessing steps in your machine-learning projects to ensure optimal results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
