{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111f1306",
   "metadata": {},
   "source": [
    "# Practice Activity: Model and Dataset Selection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Selecting the right model and dataset is crucial for ensuring the success of a fine-tuning task. Your choices at this stage will directly affect the model's performance and ability to generalise well on new data. This section will guide you through the key considerations and practical steps for choosing the appropriate model and dataset for fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## By the end of this activity, you will be able to:\n",
    "\n",
    "- Define the task requirements and select an appropriate model architecture (e.g., text classification, text generation, or question answering).\n",
    "- Choose a suitable pretrained language model for fine-tuning based on task complexity, model size, and resource constraints.\n",
    "- Select, curate, and prepare task-specific datasets that align with the model's goals, ensuring proper data quality, size, and balance.\n",
    "- Preprocess datasets by cleaning, tokenising, and balancing the data for optimal fine-tuning results.\n",
    "- Evaluate the initial model performance and refine the dataset as necessary before beginning fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Process for Model and Dataset Selection\n",
    "\n",
    "### Step 1: Define the Task and Requirements\n",
    "\n",
    "Before selecting a model or dataset, it’s essential to clearly understand your task and its specific requirements. The task type will influence the model architecture and how the dataset is prepared.\n",
    "\n",
    "#### Task Type\n",
    "\n",
    "Different tasks require different model architectures and may also require different approaches to dataset preparation. For example:\n",
    "\n",
    "- **Text classification:** Categorises text into predefined labels (e.g., spam detection, sentiment analysis). Models such as BERT and RoBERTa are well suited for these tasks due to their strong contextual understanding. For text classification, you need a dataset labelled with categories, and you must ensure that the dataset is balanced across the categories to avoid biased predictions.\n",
    "\n",
    "- **Text generation:** Allows the model to generate new text (e.g., language translation, summarisation). GPT-based models are ideal for these use cases because of their natural language generation capabilities. When preparing the dataset for text generation, it’s crucial to ensure that the input–output pairs are aligned correctly, and long sequences may need to be truncated or padded appropriately.\n",
    "\n",
    "- **Question answering:** Allows the model to extract or generate answers from text (e.g., reading comprehension). Models such as BERT and T5 are well suited for sentence-level or passage-level understanding. For question-answering tasks, you need to ensure the dataset is properly formatted, often in a question-answer pair structure with relevant context passages included. The dataset should also be cleaned to remove irrelevant information that may confuse the model.\n",
    "\n",
    "#### Task Complexity\n",
    "\n",
    "For more complex tasks, such as translating legal texts or analysing medical records, you'll need a model capable of handling domain-specific language. Fine-tuning an existing pretrained model is typically more effective than training a model from scratch. When dealing with specialised tasks, the dataset must be curated to include domain-specific terminology and balanced to cover various cases within the field.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Choose the Pretrained Model\n",
    "\n",
    "One of the most important decisions in the process is choosing a pretrained model for fine-tuning. The selected model should have a robust understanding of language and be adaptable to the task at hand.\n",
    "\n",
    "#### Commonly Used Pretrained Models\n",
    "\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):** Highly effective for such tasks as text classification, named entity recognition (NER), and question answering. It uses bidirectional attention to understand context from both sides of a word.\n",
    "\n",
    "- **GPT-3 (Generative Pretrained Transformer):** One of the most powerful models for text generation tasks, such as summarisation or language translation. It’s particularly effective at producing human-like text based on input prompts.\n",
    "\n",
    "- **RoBERTa (Robustly Optimised BERT Pretraining Approach):** Builds upon BERT but is optimised for better performance in such tasks as sentiment analysis, classification, and text completion.\n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer):** Converts all NLP tasks into a text-to-text format, making it highly versatile for translation, summarisation, and classification.\n",
    "\n",
    "#### Model Size\n",
    "\n",
    "When selecting a model, consider the available computational resources and the complexity of your task. Larger models often perform better but require significantly more memory and processing power. Smaller models can be more practical for less resource-intensive tasks or when working with smaller datasets.\n",
    "\n",
    "##### Quantifying Task Size\n",
    "\n",
    "- **Small tasks:** Typically involve datasets with fewer than 10,000 examples, short text inputs (e.g., less than 200 tokens), and relatively straightforward tasks, such as fundamental sentiment analysis or spam classification. Smaller models like BERT-base (110M parameters) or DistilBERT (66M parameters) are sufficient for such tasks. Within a few hours, these models can be fine-tuned with limited resources—such as a single GPU with 8–16GB of VRAM.\n",
    "\n",
    "  **Examples of small tasks:**\n",
    "  - Classifying customer reviews as positive or negative\n",
    "  - Detecting spam in emails with short message lengths\n",
    "  - Recognising named entities in a few hundred documents\n",
    "\n",
    "- **Large tasks:** Involve more complex datasets with at least 100,000 examples, longer text sequences (at least 500 tokens), and tasks requiring deep contextual understanding, such as question answering, machine translation, or text generation. For these tasks, larger models such as GPT-3 (175B parameters) or T5-large (770M parameters) are necessary. Fine-tuning these models requires high-end computational resources, such as multiple GPUs or TPUs with at least 32GB VRAM, and can take days or weeks, depending on the dataset size.\n",
    "\n",
    "  **Examples of large tasks:**\n",
    "  - Generating long-form text, such as articles or stories, from input prompts\n",
    "  - Answering questions based on passages from legal or medical documents\n",
    "  - Translating paragraphs of text between languages\n",
    "\n",
    "By understanding the task size, you can better gauge the appropriate model size, balancing performance and resource efficiency. For example, a small task like classifying tweets doesn’t require the computational heft of GPT-3, whereas generating detailed, coherent answers to complex questions might.\n",
    "\n",
    "#### Model Availability\n",
    "\n",
    "Before proceeding, ensure that the pretrained model you plan to fine-tune is available in your chosen framework, such as Hugging Face Transformers or TensorFlow Hub. These platforms provide access to the most popular pretrained models, prebuilt tokenisers, model configurations, and usage guides. This simplifies integration by allowing you to focus on fine-tuning rather than managing tokenisation or model architecture from scratch.\n",
    "\n",
    "#### Understanding Model Options\n",
    "\n",
    "Each model has various input options and configurable parameters, such as `num_labels` in the example below, which affects how the model performs classification (e.g., the number of categories it can classify). Understanding these options is essential to fine-tune the model for your specific task effectively. For example, in a classification task, setting `num_labels` correctly ensures the model knows how many categories to classify into.\n",
    "\n",
    "Refer to the documentation of the different models in the Hugging Face library to explore other methods, options, and parameters. This will give you detailed information on the available models, how to configure them, and the impact of different parameters on model performance. Note that we are using BERT in this example for its reliability and relative universality. Other models exist, and your employer may ask you to deploy one that is either commercially available or proprietary to the company.\n",
    "\n",
    "**Code Example (loading a pretrained model):**\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer for classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Model and tokenizer are now ready for fine-tuning\n",
    "```\n",
    "\n",
    "> **Note:** You will often find yourself in a situation in which content does not fall into a binary classification. For example, email classification’s ultimate output of “spam” or “not spam” nonetheless requires a detailed examination before `num_labels` classifies mail into its ultimate location. While this might seem as if the appropriate `num_labels` would be “2,” in this example, know that you can examine and classify items more than once — perhaps into “spam,” “not spam,” and “further review required.”\n",
    "\n",
    "To find additional configuration options and better understand how each parameter affects the model, learners can explore the respective model documentation in Hugging Face or TensorFlow Hub. These sites provide detailed examples and explanations for fine-tuning various models.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Select the Dataset\n",
    "\n",
    "Choosing the correct dataset is just as critical as selecting the model. A well-curated dataset ensures that the model learns effectively and generalises well to new tasks.\n",
    "\n",
    "### Task-Specific Datasets\n",
    "\n",
    "The dataset should be aligned with the task requirements. For example, if you are fine-tuning a model for sentiment analysis, a dataset like the IMDB movie reviews dataset labelled for positive and negative sentiments would be appropriate. For a question-answering task, the SQuAD dataset provides excellent training material.\n",
    "\n",
    "To help you explore more available datasets, such platforms as Hugging Face Datasets offer an extensive repository of task-specific datasets that you can easily integrate into your workflow. This repository includes datasets for various NLP tasks, such as text classification, generation, and translation.\n",
    "\n",
    "In addition to Hugging Face, here are other resources for finding datasets:\n",
    "\n",
    "- **Kaggle Datasets:** Kaggle provides many publicly available datasets across various domains. You can filter datasets based on task, size, and other criteria.\n",
    "- **Google Dataset Search:** Google features a search engine for datasets covering various fields, from finance to health care and beyond.\n",
    "- **UCI Machine Learning Repository:** University of California, Irvine’s (UCI) repository offers a collection of datasets for machine learning tasks, including NLP and classification tasks.\n",
    "\n",
    "Exploring these repositories allows you to find task-specific datasets that meet your fine-tuning needs. Make sure the dataset is suitable for the task and properly labelled to ensure successful model fine-tuning.\n",
    "\n",
    "### Dataset Size\n",
    "\n",
    "The size of the dataset affects how well the model will fine-tune. Larger datasets generally allow the model to learn more comprehensive patterns. However, when carefully curated and balanced, smaller datasets can also yield strong results.\n",
    "\n",
    "If you're working with a smaller dataset, consider whether it needs to be augmented to increase variety and improve model performance. Common augmentation techniques include paraphrasing (rewording the text while preserving the meaning) and backtranslation (translating the text to another language and back again to generate alternative expressions).\n",
    "\n",
    "To help you implement dataset augmentation, here are some resources and code examples:\n",
    "\n",
    "- **Hugging Face Datasets** provide built-in support for data transformations and augmentations. You can explore their documentation for augmentation techniques.\n",
    "- **nlpaug** is a popular Python library for augmenting text datasets using such methods as paraphrasing, backtranslation, and synonym replacement.\n",
    "\n",
    "#### Code Example for Augmenting Data Using Backtranslation\n",
    "\n",
    "```python\n",
    "from nlpaug.augmenter.word import BackTranslationAug\n",
    "\n",
    "# Initialize the backtranslation augmenter (English -> French -> English)\n",
    "back_translation_aug = BackTranslationAug(from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en')\n",
    "\n",
    "# Example text to augment\n",
    "text = \"The weather is great today.\"\n",
    "\n",
    "# Perform backtranslation to create augmented text\n",
    "```\n",
    "\n",
    "### Balancing the Dataset\n",
    "\n",
    "Class imbalance can significantly affect model performance. The model may become biased if certain labels are overrepresented in the dataset. To address this, you can:\n",
    "\n",
    "- Undersample the majority class to balance the dataset.\n",
    "- Oversample the minority class or generate synthetic examples using data augmentation techniques.\n",
    "- Use class weighting during model training to give more importance to underrepresented classes.\n",
    "\n",
    "**Code Example (loading a dataset for fine-tuning):**\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the IMDB movie reviews dataset for sentiment analysis\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Split the dataset into training and validation sets (if not presplit)\n",
    "train_data, val_data = train_test_split(dataset['train'], test_size=0.2)\n",
    "```\n",
    "\n",
    "### Key Concepts and Options\n",
    "\n",
    "- **Dataset splitting:** When using `train_test_split`, you can specify how much data goes into training versus validation (e.g., `test_size=0.2` allocates 20 percent of the data to validation and 80 percent to training). If the dataset is already split (as in the original example with IMDB), you don’t need to do this manually.\n",
    "\n",
    "- **Tokenisation options:** The `tokenizer` method in the code has several input options, such as `padding='max_length'` and `truncation=True`, which ensure that all sequences have the same length.\n",
    "  - **Padding:** Ensures that all input sequences are of equal length by padding shorter sequences with extra tokens.\n",
    "  - **Truncation:** Ensures that input sequences exceeding the maximum length are truncated to fit within the model’s requirements.\n",
    "\n",
    "To explore additional input options for such methods as tokenizer, learners can refer to the Hugging Face Transformers documentation, which provides detailed explanations of all available parameters.\n",
    "\n",
    "## Step 4: Preprocess the Dataset\n",
    "\n",
    "After selecting the dataset, it’s essential to preprocess the text to make it suitable for model input. Proper preprocessing ensures the data is in the right format for fine-tuning the model. Don’t worry about the specifics of data cleaning at the moment (other than broad strokes); a more detailed discussion and examples follow this element.\n",
    "\n",
    "- **Text cleaning:** Ensure the text is free from noise, such as special characters, HTML tags, or unnecessary whitespace. Depending on the model and task, text normalisation (e.g., lowercasing) may also be required.\n",
    "\n",
    "- **Tokenisation:** Use the tokenizer corresponding to the pretrained model to break down the text into input tokens. For most transformer models, tokenisation also includes padding sequences to a fixed length and truncating long sequences to ensure consistent input size.\n",
    "\n",
    "**Code Example (tokenising and preparing the dataset for training):**\n",
    "\n",
    "```python\n",
    "# Tokenise the dataset\n",
    "tokenized_train = tokenizer(\n",
    "    train_data['text'], padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_val = tokenizer(\n",
    "    val_data['text'], padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Step 5: Evaluate and Refine\n",
    "\n",
    "Once you’ve selected the model and dataset, evaluate the initial performance of the dataset before starting fine-tuning.\n",
    "\n",
    "- **Initial evaluation:** Run the pretrained model on the dataset to get a baseline performance. This helps to measure the improvements made after fine-tuning.\n",
    "\n",
    "- **Refinement:** Ensure that the dataset is balanced correctly and formatted. Before fine-tuning, you should iterate on data cleaning or dataset balancing.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Selecting and preparing the right model and dataset are crucial steps that significantly impact the success of a fine-tuning task. By clearly defining the task, choosing an appropriate pretrained model, and ensuring the dataset is high-quality and well-prepared, you lay a solid foundation for effective fine-tuning. Proper handling of imbalanced data, careful tokenisation, and thorough preprocessing ensure the model learns relevant patterns and generalises new data well. Evaluating the model’s baseline performance before fine-tuning helps maximise the benefits of the process, leading to more accurate and efficient outcomes. These careful preparations are essential for developing reliable and high-performing AI/ML systems tailored to specialised tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
