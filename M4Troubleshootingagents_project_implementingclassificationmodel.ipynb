{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1903258e",
   "metadata": {},
   "source": [
    "# Practice activity: Implementing classification models\n",
    "\n",
    "## Introduction\n",
    "In this activity, you will implement various classification models using Python. The goal is to build and evaluate such models as logistic regression, decision trees, and support vector machines (SVMs) to classify data. You will work with a dataset, preprocess the data, and train these models to see how they perform in a real-world classification task.\n",
    "\n",
    "By the end of this activity, you will be able to:\n",
    "- Describe how to preprocess data for classification tasks.\n",
    "- Implement and train multiple classification models using Python.\n",
    "- Evaluate and compare the performance of each model.\n",
    "\n",
    "## Step-by-step process\n",
    "1. **Step 1**: Set up the environment\n",
    "2. **Step 2**: Load and explore the dataset\n",
    "3. **Step 3**: Preprocess the data\n",
    "4. **Step 4**: Implement a logistic regression model\n",
    "5. **Step 5**: Implement a decision tree model\n",
    "6. **Step 6**: Implement a support vector machine model\n",
    "7. **Step 7**: Implement a Random Forest model\n",
    "8. **Step 8**: Implement a Naive Bayes model\n",
    "9. **Step 9**: Implement a Neural Network model\n",
    "10. **Step 10**: Implement a Reinforcement Learning model\n",
    "11. **Step 11**: Implement a Genetic Algorithm\n",
    "12. **Step 12**: Bayesian Networks and Markov Decision Processes\n",
    "13. **Step 13**: Evaluate and compare model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8fca3a",
   "metadata": {},
   "source": [
    "## Step 1: Set up the environment\n",
    "\n",
    "First, ensure you have the necessary libraries installed. We’ll be using Scikit-Learn for machine learning models, pandas for data manipulation, and matplotlib or seaborn for visualization.\n",
    "\n",
    "These libraries will provide the tools to load, manipulate, and visualize the dataset, as well as implement and evaluate classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1ac81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amansahni\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866d6fd",
   "metadata": {},
   "source": [
    "## Step 2: Load and explore the dataset\n",
    "\n",
    "We will use the Breast Cancer dataset from Scikit-Learn. The dataset contains features (inputs) and labels (outputs) for the classification task.\n",
    "\n",
    "Understanding the dataset helps us determine which features need to be pre-processed. We'll clean the data, handle missing values, and encode any categorical variables before training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8861ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (569, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load Breast Cancer dataset and convert to DataFrame\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef465857",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the data\n",
    "\n",
    "Preprocessing ensures that your data is clean and ready for ML models to use. Splitting the dataset into training and test sets allows us to evaluate the model’s performance on unseen data.\n",
    "\n",
    "We will:\n",
    "1. Handle missing data (if any).\n",
    "2. Split the data into training (80%) and testing (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97f6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling: 0\n",
      "Missing values after handling: 0\n",
      "Training set size: (455, 30)\n",
      "Testing set size: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before handling:\", df.isnull().sum().sum())\n",
    "\n",
    "# Handle missing data (filling missing values with the median)\n",
    "# Note: The breast cancer dataset is clean, but this step is crucial for real-world datasets\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "print(\"Missing values after handling:\", df.isnull().sum().sum())\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e17c2b",
   "metadata": {},
   "source": [
    "## Step 4: Implement a logistic regression model\n",
    "\n",
    "Logistic regression is a simple yet effective model for binary classification tasks. It models the probability that a given input belongs to a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a9f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train logistic regression model\n",
    "# Increasing max_iter to ensure convergence for this dataset\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5116c",
   "metadata": {},
   "source": [
    "## Step 5: Implement a decision tree model\n",
    "\n",
    "Decision trees split the data based on feature values and make decisions at each node. They are highly interpretable but can be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3bd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train decision tree model\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c79f55",
   "metadata": {},
   "source": [
    "## Step 6: Implement a support vector machine model\n",
    "\n",
    "SVMs are powerful models, particularly in high-dimensional spaces. They work by finding a hyperplane that separates data points into different classes with the maximum margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123b44ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM model\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23ff92",
   "metadata": {},
   "source": [
    "## Step 7: Implement a Random Forest model\n",
    "\n",
    "Random Forest is an ensemble method that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f81285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f470c6f",
   "metadata": {},
   "source": [
    "## Step 8: Implement a Naive Bayes model\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab56b3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905d882",
   "metadata": {},
   "source": [
    "## Step 9: Implement a Neural Network model\n",
    "\n",
    "Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function by training on a dataset. It can learn a non-linear function approximator for either classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c21065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train Neural Network model\n",
    "nn = MLPClassifier(max_iter=1000, random_state=42)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn.predict(X_test)\n",
    "\n",
    "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbc11b",
   "metadata": {},
   "source": [
    "## Step 10: Implement a Reinforcement Learning model (Simple Q-Learning)\n",
    "\n",
    "Reinforcement Learning (RL) involves an agent learning to make decisions by performing actions and receiving rewards. Here, we implement a simplified **Q-Learning** inspired classifier. The \"agent\" looks at the data (state), predicts a class (action), and receives a reward (+1 for correct, -1 for incorrect). It updates its internal weights to maximize future rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154fdfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Agent Accuracy: 0.37719298245614036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRLClassifier:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        \n",
    "        # Convert DataFrame to numpy if needed\n",
    "        X_arr = np.array(X)\n",
    "        y_arr = np.array(y)\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for idx, x_i in enumerate(X_arr):\n",
    "                # Action: Predict Class 1 if dot product > 0, else Class 0\n",
    "                prediction = 1 if np.dot(x_i, self.weights) >= 0 else 0\n",
    "                \n",
    "                # Reward: +1 if correct, -1 if incorrect\n",
    "                reward = 1 if prediction == y_arr[idx] else -1\n",
    "                \n",
    "                # Update weights (Simplified Q-learning / Perceptron rule)\n",
    "                # If prediction is wrong, move weights towards the correct direction\n",
    "                if prediction != y_arr[idx]:\n",
    "                    self.weights += self.lr * reward * x_i\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_arr = np.array(X)\n",
    "        return np.where(np.dot(X_arr, self.weights) >= 0, 1, 0)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return {\"learning_rate\": self.lr, \"n_iterations\": self.n_iter}\n",
    "\n",
    "# Train RL model\n",
    "rl_agent = SimpleRLClassifier(learning_rate=0.01, n_iterations=50)\n",
    "rl_agent.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rl = rl_agent.predict(X_test)\n",
    "\n",
    "print(\"RL Agent Accuracy:\", accuracy_score(y_test, y_pred_rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b3a1a",
   "metadata": {},
   "source": [
    "## Step 11: Implement a Genetic Algorithm (for Feature Selection)\n",
    "\n",
    "Genetic Algorithms (GA) are used for optimization. In this example, we use a GA to find the **best subset of features** for a Logistic Regression model.\n",
    "1.  **Population**: A set of random feature masks (binary vectors).\n",
    "2.  **Fitness**: The accuracy of a model trained with the selected features.\n",
    "3.  **Crossover/Mutation**: Combining and tweaking masks to create better ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2797f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic Algorithm Optimized Accuracy: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class GeneticAlgorithmFeatureSelector:\n",
    "    def __init__(self, n_generations=5, population_size=10, mutation_rate=0.1):\n",
    "        self.n_generations = n_generations\n",
    "        self.pop_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.best_mask = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test):\n",
    "        n_features = X.shape[1]\n",
    "        # Initialize population (random binary masks)\n",
    "        population = [[random.randint(0, 1) for _ in range(n_features)] for _ in range(self.pop_size)]\n",
    "\n",
    "        for gen in range(self.n_generations):\n",
    "            # Evaluate fitness (Accuracy)\n",
    "            scores = []\n",
    "            for mask in population:\n",
    "                if sum(mask) == 0: mask[0] = 1 # Ensure at least one feature\n",
    "                selected_features = [i for i, bit in enumerate(mask) if bit]\n",
    "                \n",
    "                clf = LogisticRegression(max_iter=2000)\n",
    "                clf.fit(X.iloc[:, selected_features], y)\n",
    "                score = clf.score(X_test.iloc[:, selected_features], y_test)\n",
    "                scores.append((score, mask, clf))\n",
    "            \n",
    "            # Sort by score\n",
    "            scores.sort(key=lambda x: x[0], reverse=True)\n",
    "            self.best_mask = scores[0][1]\n",
    "            self.best_model = scores[0][2]\n",
    "            \n",
    "            # Selection & Crossover (Simple)\n",
    "            top_half = [x[1] for x in scores[:self.pop_size//2]]\n",
    "            new_population = top_half[:]\n",
    "            \n",
    "            while len(new_population) < self.pop_size:\n",
    "                parent1, parent2 = random.sample(top_half, 2)\n",
    "                split = random.randint(0, n_features-1)\n",
    "                child = parent1[:split] + parent2[split:]\n",
    "                # Mutation\n",
    "                if random.random() < self.mutation_rate:\n",
    "                    idx = random.randint(0, n_features-1)\n",
    "                    child[idx] = 1 - child[idx]\n",
    "                new_population.append(child)\n",
    "            \n",
    "            population = new_population\n",
    "\n",
    "    def predict(self, X):\n",
    "        selected_features = [i for i, bit in enumerate(self.best_mask) if bit]\n",
    "        return self.best_model.predict(X.iloc[:, selected_features])\n",
    "\n",
    "    def get_params(self):\n",
    "        return {\"n_generations\": self.n_generations, \"pop_size\": self.pop_size}\n",
    "\n",
    "# Train GA model\n",
    "ga_model = GeneticAlgorithmFeatureSelector(n_generations=5, population_size=10)\n",
    "ga_model.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ga = ga_model.predict(X_test)\n",
    "\n",
    "print(\"Genetic Algorithm Optimized Accuracy:\", accuracy_score(y_test, y_pred_ga))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01657191",
   "metadata": {},
   "source": [
    "## Step 12: Bayesian Networks and Markov Decision Processes\n",
    "\n",
    "### Bayesian Networks\n",
    "A Bayesian Network represents variables and their conditional dependencies.\n",
    "*   **Note**: The **Naive Bayes** model we implemented in **Step 8** is actually a specific, simple type of Bayesian Network where we assume all features are independent given the class label.\n",
    "*   **Code**: See Step 8 for the implementation.\n",
    "\n",
    "### Markov Decision Processes (MDP)\n",
    "MDPs are used for sequential decision-making, not typically for static classification datasets like this one.\n",
    "*   **Concept**: An agent moves between states ($S$) by taking actions ($A$), receiving rewards ($R$), and transitioning based on probabilities ($P$).\n",
    "*   **Example Code (Toy)**: Below is a simple class structure showing how an MDP is defined, though we won't apply it to the Breast Cancer dataset as it doesn't fit the problem structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f954529d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Structure defined. (Not applicable for static classification)\n"
     ]
    }
   ],
   "source": [
    "# Toy Example of an MDP structure (Not applied to Breast Cancer data)\n",
    "class SimpleMDP:\n",
    "    def __init__(self, states, actions, transition_probs, rewards):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.T = transition_probs # P(s' | s, a)\n",
    "        self.R = rewards          # R(s, a, s')\n",
    "\n",
    "    def get_policy(self):\n",
    "        # Placeholder for Value Iteration or Policy Iteration algorithm\n",
    "        return \"Optimal Policy would be calculated here\"\n",
    "\n",
    "# Example usage (Conceptual)\n",
    "mdp = SimpleMDP(states=['Healthy', 'Sick'], actions=['Treat', 'Wait'], transition_probs={}, rewards={})\n",
    "print(\"MDP Structure defined. (Not applicable for static classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481cb82",
   "metadata": {},
   "source": [
    "## Step 13: Evaluate and compare model performance\n",
    "\n",
    "We will now create a comprehensive table comparing all the models we have implemented. We will look at Accuracy, Precision, Recall, and F1 Score. We will also list key training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6b9bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amansahni\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Data Scanned (Rows)</th>\n",
       "      <th>Key Training Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.956905</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.955801</td>\n",
       "      <td>569</td>\n",
       "      <td>C=1.0, solver=lbfgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>569</td>\n",
       "      <td>criterion=gini, max_depth=None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.951470</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.946462</td>\n",
       "      <td>569</td>\n",
       "      <td>C=1.0, kernel=rbf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.965205</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.964738</td>\n",
       "      <td>569</td>\n",
       "      <td>n_estimators=100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.974751</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.973481</td>\n",
       "      <td>569</td>\n",
       "      <td>var_smoothing=1e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.944107</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>0.937318</td>\n",
       "      <td>569</td>\n",
       "      <td>hidden_layer_sizes=(100,), activation=relu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinforcement Learning (Simple)</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>0.142275</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>0.206615</td>\n",
       "      <td>569</td>\n",
       "      <td>lr=0.01, iter=50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Genetic Algorithm (Feature Sel.)</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.991350</td>\n",
       "      <td>0.991228</td>\n",
       "      <td>0.991207</td>\n",
       "      <td>569</td>\n",
       "      <td>gens=5, pop=10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model  Accuracy  Precision    Recall  F1 Score  \\\n",
       "0               Logistic Regression  0.956140   0.956905  0.956140  0.955801   \n",
       "1                     Decision Tree  0.947368   0.947368  0.947368  0.947368   \n",
       "2                               SVM  0.947368   0.951470  0.947368  0.946462   \n",
       "3                     Random Forest  0.964912   0.965205  0.964912  0.964738   \n",
       "4                       Naive Bayes  0.973684   0.974751  0.973684  0.973481   \n",
       "5                    Neural Network  0.938596   0.944107  0.938596  0.937318   \n",
       "6   Reinforcement Learning (Simple)  0.377193   0.142275  0.377193  0.206615   \n",
       "7  Genetic Algorithm (Feature Sel.)  0.991228   0.991350  0.991228  0.991207   \n",
       "\n",
       "   Data Scanned (Rows)                         Key Training Params  \n",
       "0                  569                         C=1.0, solver=lbfgs  \n",
       "1                  569              criterion=gini, max_depth=None  \n",
       "2                  569                           C=1.0, kernel=rbf  \n",
       "3                  569                            n_estimators=100  \n",
       "4                  569                         var_smoothing=1e-09  \n",
       "5                  569  hidden_layer_sizes=(100,), activation=relu  \n",
       "6                  569                            lr=0.01, iter=50  \n",
       "7                  569                              gens=5, pop=10  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Dictionary of models and their predictions\n",
    "models_results = [\n",
    "    (\"Logistic Regression\", log_reg, y_pred_log),\n",
    "    (\"Decision Tree\", tree, y_pred_tree),\n",
    "    (\"SVM\", svm, y_pred_svm),\n",
    "    (\"Random Forest\", rf, y_pred_rf),\n",
    "    (\"Naive Bayes\", nb, y_pred_nb),\n",
    "    (\"Neural Network\", nn, y_pred_nn),\n",
    "    (\"Reinforcement Learning (Simple)\", rl_agent, y_pred_rl),\n",
    "    (\"Genetic Algorithm (Feature Sel.)\", ga_model, y_pred_ga)\n",
    "]\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for name, model, y_pred in models_results:\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Extract key parameters\n",
    "    params = model.get_params()\n",
    "    if \"Logistic\" in name:\n",
    "        key_params = f\"C={params.get('C')}, solver={params.get('solver')}\"\n",
    "    elif \"Decision Tree\" in name:\n",
    "        key_params = f\"criterion={params.get('criterion')}, max_depth={params.get('max_depth')}\"\n",
    "    elif \"SVM\" in name:\n",
    "        key_params = f\"C={params.get('C')}, kernel={params.get('kernel')}\"\n",
    "    elif \"Random Forest\" in name:\n",
    "        key_params = f\"n_estimators={params.get('n_estimators')}\"\n",
    "    elif \"Naive Bayes\" in name:\n",
    "        key_params = f\"var_smoothing={params.get('var_smoothing')}\"\n",
    "    elif \"Neural Network\" in name:\n",
    "        key_params = f\"hidden_layer_sizes={params.get('hidden_layer_sizes')}, activation={params.get('activation')}\"\n",
    "    elif \"Reinforcement\" in name:\n",
    "        key_params = f\"lr={params.get('learning_rate')}, iter={params.get('n_iterations')}\"\n",
    "    elif \"Genetic\" in name:\n",
    "        key_params = f\"gens={params.get('n_generations')}, pop={params.get('pop_size')}\"\n",
    "    else:\n",
    "        key_params = \"N/A\"\n",
    "\n",
    "    results_data.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Data Scanned (Rows)\": len(X),\n",
    "        \"Key Training Params\": key_params\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display the table\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538df8b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this activity, you successfully implemented a wide range of classification and optimization models using Python. You started with fundamental models like **Logistic Regression**, **Decision Trees**, and **SVMs**, and progressed to more advanced techniques including **Random Forests**, **Naive Bayes**, and **Neural Networks**.\n",
    "\n",
    "Furthermore, you explored how **Reinforcement Learning** concepts can be applied to classification, used **Genetic Algorithms** for feature selection, and learned about the theoretical foundations of **Bayesian Networks** and **Markov Decision Processes**.\n",
    "\n",
    "By training, evaluating, and comparing these diverse models, you gained a comprehensive understanding of the machine learning landscape and how different algorithms approach the task of classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
